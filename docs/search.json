[
  {
    "objectID": "projects/04_Premiere League AI Assistant/index.html",
    "href": "projects/04_Premiere League AI Assistant/index.html",
    "title": "Premiere League AI Assistant ",
    "section": "",
    "text": "An AI-driven assistant that brings the Premier League to life through data and interaction.It pulls in match data, processes it, builds insights, and wraps everything into a chatbot that can answer your football-related queries.\n\n\n\n\n\nFetch Data â€“ Connects to live Premier League APIs and gathers real-time stats.\nPreprocess â€“ Cleans and organizes the incoming data so itâ€™s ready to use.\nBuild Features â€“ Extracts key insights from the stats (players, matches, trends).\nRecommend â€“ Suggests players, teams, or match highlights based on the data.\nChatbot â€“ Lets users interact directly, ask questions, and get quick, football-focused answers.\n\n\n\n\n\n\nLanguage: Python\nAPIs: Integration with live Premier League APIs for up-to-date data\nData Engineering: Cleaning, preprocessing, and feature building\nConversational AI: A chatbot interface for user interaction and Q&A about football\n\n\n\n\n\nUses real-time data, not just static datasets.\nCovers the full pipeline: from API integration â†’ data cleaning â†’ insights â†’ Q&A chatbot.\nOrganized into clear modules, making it easy to expand with new features.\nShows how football data can be made interactive and engaging for fans."
  },
  {
    "objectID": "projects/04_Premiere League AI Assistant/index.html#what-the-project-does",
    "href": "projects/04_Premiere League AI Assistant/index.html#what-the-project-does",
    "title": "Premiere League AI Assistant ",
    "section": "",
    "text": "Fetch Data â€“ Connects to live Premier League APIs and gathers real-time stats.\nPreprocess â€“ Cleans and organizes the incoming data so itâ€™s ready to use.\nBuild Features â€“ Extracts key insights from the stats (players, matches, trends).\nRecommend â€“ Suggests players, teams, or match highlights based on the data.\nChatbot â€“ Lets users interact directly, ask questions, and get quick, football-focused answers."
  },
  {
    "objectID": "projects/04_Premiere League AI Assistant/index.html#tech-stack-skills-used",
    "href": "projects/04_Premiere League AI Assistant/index.html#tech-stack-skills-used",
    "title": "Premiere League AI Assistant ",
    "section": "",
    "text": "Language: Python\nAPIs: Integration with live Premier League APIs for up-to-date data\nData Engineering: Cleaning, preprocessing, and feature building\nConversational AI: A chatbot interface for user interaction and Q&A about football"
  },
  {
    "objectID": "projects/04_Premiere League AI Assistant/index.html#what-makes-it-interesting",
    "href": "projects/04_Premiere League AI Assistant/index.html#what-makes-it-interesting",
    "title": "Premiere League AI Assistant ",
    "section": "",
    "text": "Uses real-time data, not just static datasets.\nCovers the full pipeline: from API integration â†’ data cleaning â†’ insights â†’ Q&A chatbot.\nOrganized into clear modules, making it easy to expand with new features.\nShows how football data can be made interactive and engaging for fans."
  },
  {
    "objectID": "projects/02_turf/index.html",
    "href": "projects/02_turf/index.html",
    "title": "Fast TURF with RCPP",
    "section": "",
    "text": "TURF (Total Unduplicated Reach and Frequency) is a marketing analytics technique used to identify the optimal combination of items (e.g., products, features, messages) that maximizes audience reach. It helps businesses decide which subset of options can collectively appeal to the largest number of unique customers."
  },
  {
    "objectID": "projects/02_turf/index.html#what-is-turf",
    "href": "projects/02_turf/index.html#what-is-turf",
    "title": "Fast TURF with RCPP",
    "section": "",
    "text": "TURF (Total Unduplicated Reach and Frequency) is a marketing analytics technique used to identify the optimal combination of items (e.g., products, features, messages) that maximizes audience reach. It helps businesses decide which subset of options can collectively appeal to the largest number of unique customers."
  },
  {
    "objectID": "projects/02_turf/index.html#previous-approach-at-gbk",
    "href": "projects/02_turf/index.html#previous-approach-at-gbk",
    "title": "Fast TURF with RCPP",
    "section": "Previous Approach at GBK",
    "text": "Previous Approach at GBK\nThey were using R code with lots of loops to calculate all possible combinations for TURF analysis. While this approach works for small datasets, it becomes very slow and inefficient when there are many products or respondents.\nWhy? Because, R loops are not the fastest â€” they go one step at a time.\nTURF needs to test millions of combinations, especially when selecting the best 5 out of 20+ items.\nAs the number of items grows, the number of combinations explodes, making the R code much slower.\nThis made it hard to work with larger datasets or run multiple TURF analyses quickly."
  },
  {
    "objectID": "projects/02_turf/index.html#my-solution",
    "href": "projects/02_turf/index.html#my-solution",
    "title": "Fast TURF with RCPP",
    "section": "My Solution",
    "text": "My Solution\n\nThe C++ Code:\nI rewrote the critical portions in C++ using Rcpp to drastically reduce computation time and memory usage.\n\n#include &lt;Rcpp.h&gt;\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nDataFrame turf_m(NumericMatrix mat, int m, int keep = 10, SEXP w = R_NilValue) {\n  int N = mat.nrow(), J = mat.ncol();\n  if (m &lt; 1 || m &gt; J) stop(\"m must be between 1 and ncol(mat)\");\n  \n  // Handle weights\n  NumericVector weights = (Rf_isNull(w) || (Rf_length(w) == 1 && as&lt;double&gt;(w) == 1.0))\n    ? NumericVector(N, 1.0)\n      : as&lt;NumericVector&gt;(w);\n  \n  if (weights.size() != N || is_true(any(weights &lt; 0)) || sum(weights) == 0 ||\n      is_true(any(!is_finite(weights))) || is_true(any(is_na(weights))))\n    stop(\"Invalid weights\");\n  \n  // Generate combinations of m columns\n  IntegerMatrix combos = transpose(as&lt;IntegerMatrix&gt;(Function(\"combn\")(J, m)));\n  int M = combos.nrow();\n  \n  NumericVector reach(M), freq(M);\n  for (int i = 0; i &lt; M; ++i) {\n    NumericVector y(N);\n    for (int j = 0; j &lt; m; ++j) {\n      int col = combos(i, j) - 1;\n      for (int n = 0; n &lt; N; ++n)\n        y[n] += mat(n, col);\n    }\n    \n    double rsum = 0, fsum = 0, wsum = sum(weights);\n    for (int n = 0; n &lt; N; ++n) {\n      if (y[n] &gt; 0) rsum += weights[n];\n      fsum += y[n] * weights[n];\n    }\n    \n    reach[i] = rsum / wsum;\n    freq[i]  = fsum / wsum;\n  }\n  \n  // Sort by reach and keep top combos\n  IntegerVector ord = as&lt;IntegerVector&gt;(Function(\"order\")(reach, _[\"decreasing\"] = true));\n  int n = std::min(M, keep);\n  ord = ord[Range(0, n - 1)];\n  \n  // Build output\n  List out = List::create(\n    _[\"size\"] = rep(m, n),\n    _[\"rank\"] = seq_len(n),\n    _[\"reach\"] = reach[ord - 1],\n                      _[\"freq\"]  = freq[ord - 1]\n  );\n  \n  for (int j = 0; j &lt; m; ++j) {\n    IntegerVector items(n);\n    for (int i = 0; i &lt; n; ++i)\n      items[i] = combos(ord[i] - 1, j);\n    out[\"item\" + std::to_string(j + 1)] = items;\n  }\n  \n  return as&lt;DataFrame&gt;(out);\n}\n\n// [[Rcpp::export]]\nDataFrame turf(NumericMatrix mat, int j = -1, int keep = 10, SEXP w = R_NilValue) {\n  if (j == -1) j = std::min(5, mat.ncol() - 1);\n  \n  List res;\n  for (int m = 1; m &lt;= j; ++m)\n    res.push_back(turf_m(mat, m, keep, w));\n  \n  return as&lt;DataFrame&gt;(Function(\"rbindlist\", Environment::namespace_env(\"data.table\"))(res, Named(\"fill\") = true));\n}\n\nLet me show you how much faster it is:\n\nlibrary(Rcpp)\nlibrary(microbenchmark)\n\n# Compile the C++ function\nsourceCpp(\"turf_rcpp.cpp\")\n\n# Load sample data\ndata &lt;- as.matrix(read.csv(\"out_mat1_forSB.csv\"))\n\n# Run TURF using optimized function\nresult &lt;- turf(data, j = 5, keep = 10)\nprint(result)\n\n# Benchmark performance\nsystem.time(result_og &lt;- turf(data))\n\nmicrobenchmark(\n  turf(data, j = 5, keep = 10),\n  times = 10\n)"
  },
  {
    "objectID": "certifications/certificates.html",
    "href": "certifications/certificates.html",
    "title": "My Certifications",
    "section": "",
    "text": "ðŸŽ“ My Certifications\n\n\nIBM Data Science Professional Certificate\n\n\n\nWhat is Data Science (PDF)\nTools for Data Science (PDF)\nData Science Methodology (PDF)\nData Analysis with Python (PDF)\nDatabases and SQL with Python (PDF)\nMachine Learning with Python (PDF)\nGenerative AI (PDF)\n\n\n\n\nGenerative AI Engineering with LLMs Specialization\n\n\n\n1. Architecture and Data (PDF)\n2. Gen AI Foundational Models for NLP & Language (PDF)\n3. Generative AI Language Modeling (PDF)\n4. Generative AI Engineering and Fine-Tuning Transformers (PDF)\n5. Generative AI Advanced Fine-Tuning for LLMs (PDF)\n6. Fundamentals of AI Agents Using RAG and LangChain (PDF)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Shraddheshâ€™s Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shraddhesh Bhalerao",
    "section": "",
    "text": "Iâ€™m a Data Science enthusiast with a strong foundation in Machine learning, AI, and analytics. My work includes AI assistants, segmentation frameworks, and C++-accelerated simulation tools that drive real-world impact.\n\n\n\nBuilt a GPT-4o-RAG based AI Segmentation Assistant to automate cluster analysis.\nApplied PCA and KMeans to segment customers and surface insights for targeted marketing.\nRewrote slow R code in C++ with Rcpp, achieving 5Ã— speedups in TURF simulations.\n\n\n\n\n\n\nData Science Intern | GBK Collective, USA (Feb 2025 - August 2025)\n\n\n\n\n\n\nB.Tech, Computer Science & Engineering Symbiosis Institute of Technology, Pune â€“ June 2021â€“June 2025\nJunior College Atomic Energy Central School No.1, Tarapur â€“ April 2019â€“June 2021"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Shraddhesh Bhalerao",
    "section": "",
    "text": "Data Science Intern | GBK Collective, USA (Feb 2025 - August 2025)"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Shraddhesh Bhalerao",
    "section": "",
    "text": "B.Tech, Computer Science & Engineering Symbiosis Institute of Technology, Pune â€“ June 2021â€“June 2025\nJunior College Atomic Energy Central School No.1, Tarapur â€“ April 2019â€“June 2021"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Customer Segmentation with PCA & KMeans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFast TURF with RCPP\n\n\nOptimized a TURF model by rewriting it in C++ using Rcpp, achieving major speed-ups compared to base R.\n\n\n\n\n\n\n\n\n\n\n\n\nPremiere League AI Assistant \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSleep Disorder Prediction \n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/01_segmentation/index.html",
    "href": "projects/01_segmentation/index.html",
    "title": "Customer Segmentation with PCA & KMeans",
    "section": "",
    "text": "Performed customer segmentation by applying PCA for dimensionality reduction followed by KMeans clustering. Explored cluster counts from 2 to 6 and evaluated the results using both the Elbow Method and silhouette scores to ensure well-separated groupings. The analysis uncovered distinct clusters characterized by differences in shopping habits, demographics, and financial behavior, providing actionable insights for targeted marketing strategies."
  },
  {
    "objectID": "projects/01_segmentation/index.html#project-overview",
    "href": "projects/01_segmentation/index.html#project-overview",
    "title": "Customer Segmentation with PCA & KMeans",
    "section": "",
    "text": "Performed customer segmentation by applying PCA for dimensionality reduction followed by KMeans clustering. Explored cluster counts from 2 to 6 and evaluated the results using both the Elbow Method and silhouette scores to ensure well-separated groupings. The analysis uncovered distinct clusters characterized by differences in shopping habits, demographics, and financial behavior, providing actionable insights for targeted marketing strategies."
  },
  {
    "objectID": "projects/01_segmentation/index.html#my-contributions",
    "href": "projects/01_segmentation/index.html#my-contributions",
    "title": "Customer Segmentation with PCA & KMeans",
    "section": "My Contributions",
    "text": "My Contributions\n\nPerformed data cleaning and exploratory analysis.\nExecuted PCA for dimensionality reduction.\nBuilt KMeans clustering models.\nVisualized clustering outcomes and interpreted customer segments."
  },
  {
    "objectID": "projects/01_segmentation/index.html#technologies-used",
    "href": "projects/01_segmentation/index.html#technologies-used",
    "title": "Customer Segmentation with PCA & KMeans",
    "section": "Technologies Used",
    "text": "Technologies Used\n\nPython (Pandas, NumPy, Scikit-learn, Matplotlib, Seaborn)\nJupyter Notebook"
  },
  {
    "objectID": "projects/01_segmentation/index.html#code",
    "href": "projects/01_segmentation/index.html#code",
    "title": "Customer Segmentation with PCA & KMeans",
    "section": "Code",
    "text": "Code\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.cluster import KMeans\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\n\nfile_path = \"Segmented_Data_PCA_KMeans.xlsx\"\ndf = pd.read_excel(file_path)\n\n\nsegmentation_vars = [\"Age\", \"Gender\", \"Shopping_on_Amazon\", \"annual_income_US\", \"Parent\", \"Credit\"]\ndf_segment = df[segmentation_vars]\n\n\nlabel_encoders = {}\ncategorical_vars = [\"Gender\", \"Shopping_on_Amazon\", \"Parent\", \"Credit\"]\n\nfor col in categorical_vars:\n    le = LabelEncoder()\n    df_segment[col] = le.fit_transform(df_segment[col].astype(str))\n    label_encoders[col] = le\n\n\ndf_segment[\"annual_income_US\"].fillna(df_segment[\"annual_income_US\"].median(), inplace=True)\n\n\ndf_segment[\"annual_income_US\"].fillna(df_segment[\"annual_income_US\"].median(), inplace=True)\n\n\nprint(df_segment.isna().sum())\n\npca = PCA(n_components=2) \ndf_pca = pca.fit_transform(df_segment) \ndf_pca = pd.DataFrame(df_pca, columns=[\"PC1\", \"PC2\"]) \nprint(df_pca)\n\n\nwcss = []\nsilhouette_scores = []\n\nfor k in range(2, 7): \n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    labels = kmeans.fit_predict(df_pca)\n    wcss.append(kmeans.inertia_)\n    silhouette_scores.append(silhouette_score(df_pca, labels))\n    \nplt.figure(figsize=(8, 5))\nplt.plot(range(2, 7), wcss, marker=\"o\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"WCSS\")\nplt.title(\"Elbow Method for Optimal K\")\nplt.show()\n\nfor i, k in enumerate(range(2, 7)):\n    print(f\"K={k}, Silhouette Score={silhouette_scores[i]:.4f}\")\n\n\noptimal_k = 3 \nkmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\ndf[\"Segment\"] = kmeans.fit_predict(df_pca)\ndf.to_excel(\"Segmented_Data_PCA_KMeans.xlsx\", index=False)\n\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=df_pca[\"PC1\"], y=df_pca[\"PC2\"], hue=df[\"Segment\"], palette=\"viridis\")\nplt.title(\" K=3\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.legend(title=\"Cluster\")\nplt.show()"
  },
  {
    "objectID": "projects/01_segmentation/index.html#output",
    "href": "projects/01_segmentation/index.html#output",
    "title": "Customer Segmentation with PCA & KMeans",
    "section": "Output",
    "text": "Output\n\nprint(df.groupby(\"Segment\").mean())\n\n\nsegment_summary = df.groupby(\"Segment\").mean()\n\nfrom IPython.display import display\ndisplay(segment_summary)\n\n\nsegment_counts = df[\"Segment\"].value_counts()\ntotal_count = len(df)\n\nsegment_percentage = (segment_counts / total_count) * 100\nsegment_percentage_df = segment_percentage.reset_index()\nsegment_percentage_df.columns = [\"Segment\", \"Percentage\"]\ndisplay(segment_percentage_df)\n\n\npercent_by_gender = pd.crosstab(\n    df[\"Gender\"], \n    df[\"Segment\"], \n    normalize=\"index\"\n) * 100\n\nprint(\"Row-Wise \")\ndisplay(percent_by_gender)\npercent_by_segment = pd.crosstab(\n    df[\"Gender\"], \n    df[\"Segment\"], \n    normalize=\"columns\"\n) * 100\n\nprint(\"Column-Wise \")\ndisplay(percent_by_segment)\ncounts_with_totals = pd.crosstab(\n    df[\"Gender\"], \n    df[\"Segment\"], \n    margins=True\n)\ndisplay(counts_with_totals)"
  },
  {
    "objectID": "projects/03_Sleep_disorder_prediction/index.html",
    "href": "projects/03_Sleep_disorder_prediction/index.html",
    "title": "Sleep Disorder Prediction ",
    "section": "",
    "text": "The objective of this data science project is to analyze lifestyle and medical variablesâ€“such as age, BMI, physical activity, sleep duration, and blood pressureâ€“to predict the presence and type of sleep disorder (e.g., Insomnia, Sleep Apnea). Early identification of individuals at risk can enable targeted interventions to improve sleep quality and health outcomes.\nIt is a machine learning project implemented in Python (Jupyter Notebook) using scikit-learn to train models that predict sleep disorders based on lifestyle and health metrics.\n\n\nThe workflow is clear and modular:\n\nFetch Data â€“ Uses a curated dataset of health and lifestyle indicators.\nPreprocess â€“ Cleans and organizes the raw data (handling missing values, encoding categories, normalization).\nBuild Features â€“ Extracts meaningful predictors such as age, BMI, sleep duration, stress levels, and heart rate.\nTrain Models â€“ Applies algorithms like Logistic Regression, Random Forest, and KNN to classify sleep disorder presence.\nEvaluate â€“ Compares performance using metrics such as accuracy, precision, recall, and F1-score to determine the best model.\n\n\n\n\n\nLanguage: Python\nLibraries: Pandas, NumPy, scikit-learn, Matplotlib, Seaborn\nData Engineering: Cleaning, preprocessing, feature engineering\nMachine Learning: Supervised classification (Logistic Regression, Random Forest, KNN, etc.)\nVisualization: Charts and graphs to highlight trends and correlations\n\n\n\n\n\nWorks on real-world health data, not synthetic samples.\nCovers the end-to-end pipeline: dataset â†’ preprocessing â†’ feature extraction â†’ ML model â†’ evaluation.\nIdentifies key lifestyle factors (like BMI, stress, and sleep duration) that strongly influence sleep disorders.\nOrganized modularly, making it easy to extend with new models or datasets.\n\n\n\n\n\nCorrelation Example: Higher BMI and lower sleep duration are strongly correlated with insomnia.\nModel Results: Random Forest achieved the highest accuracy (89%).\nVisuals: Feature importance graphs and correlation heatmaps highlight the strongest predictors."
  },
  {
    "objectID": "projects/03_Sleep_disorder_prediction/index.html#what-the-project-does",
    "href": "projects/03_Sleep_disorder_prediction/index.html#what-the-project-does",
    "title": "Sleep Disorder Prediction ",
    "section": "",
    "text": "The workflow is clear and modular:\n\nFetch Data â€“ Uses a curated dataset of health and lifestyle indicators.\nPreprocess â€“ Cleans and organizes the raw data (handling missing values, encoding categories, normalization).\nBuild Features â€“ Extracts meaningful predictors such as age, BMI, sleep duration, stress levels, and heart rate.\nTrain Models â€“ Applies algorithms like Logistic Regression, Random Forest, and KNN to classify sleep disorder presence.\nEvaluate â€“ Compares performance using metrics such as accuracy, precision, recall, and F1-score to determine the best model."
  },
  {
    "objectID": "projects/03_Sleep_disorder_prediction/index.html#tech-stack-skills-used",
    "href": "projects/03_Sleep_disorder_prediction/index.html#tech-stack-skills-used",
    "title": "Sleep Disorder Prediction ",
    "section": "",
    "text": "Language: Python\nLibraries: Pandas, NumPy, scikit-learn, Matplotlib, Seaborn\nData Engineering: Cleaning, preprocessing, feature engineering\nMachine Learning: Supervised classification (Logistic Regression, Random Forest, KNN, etc.)\nVisualization: Charts and graphs to highlight trends and correlations"
  },
  {
    "objectID": "projects/03_Sleep_disorder_prediction/index.html#what-makes-it-interesting",
    "href": "projects/03_Sleep_disorder_prediction/index.html#what-makes-it-interesting",
    "title": "Sleep Disorder Prediction ",
    "section": "",
    "text": "Works on real-world health data, not synthetic samples.\nCovers the end-to-end pipeline: dataset â†’ preprocessing â†’ feature extraction â†’ ML model â†’ evaluation.\nIdentifies key lifestyle factors (like BMI, stress, and sleep duration) that strongly influence sleep disorders.\nOrganized modularly, making it easy to extend with new models or datasets."
  },
  {
    "objectID": "projects/03_Sleep_disorder_prediction/index.html#example-insights",
    "href": "projects/03_Sleep_disorder_prediction/index.html#example-insights",
    "title": "Sleep Disorder Prediction ",
    "section": "",
    "text": "Correlation Example: Higher BMI and lower sleep duration are strongly correlated with insomnia.\nModel Results: Random Forest achieved the highest accuracy (89%).\nVisuals: Feature importance graphs and correlation heatmaps highlight the strongest predictors."
  },
  {
    "objectID": "certifications/certificates.html#ibm-data-science-professional-certificate",
    "href": "certifications/certificates.html#ibm-data-science-professional-certificate",
    "title": "My Certifications",
    "section": "",
    "text": "What is Data Science\nTools for Data Science\nData Science Methodology\nData Analysis with Python\nDatabases and SQL with Python\nMachine Learning with Python\nGenerative AI"
  },
  {
    "objectID": "certifications/certificates.html#generative-ai-engineering-with-llms-specialization",
    "href": "certifications/certificates.html#generative-ai-engineering-with-llms-specialization",
    "title": "My Certifications",
    "section": "",
    "text": "1. Architecture and Data\n2. Gen AI Foundational Models for NLP & Language\n3. Generative AI Language Modeling\n4. Generative AI Engineering and Fine-Tuning Transformers\n5. Generative AI Advanced Fine-Tuning for LLMs\n6. Fundamentals of AI Agents Using RAG and LangChain"
  },
  {
    "objectID": "certifications/certificates.html#uibm-data-science-professional-certificateu",
    "href": "certifications/certificates.html#uibm-data-science-professional-certificateu",
    "title": "ðŸŽ“My Certifications",
    "section": "",
    "text": "What is Data Science\nTools for Data Science\nData Science Methodology\nData Analysis with Python\nDatabases and SQL with Python\nMachine Learning with Python\nGenerative AI"
  },
  {
    "objectID": "certifications/certificates.html#ugenerative-ai-engineering-with-llms-specializationu",
    "href": "certifications/certificates.html#ugenerative-ai-engineering-with-llms-specializationu",
    "title": "ðŸŽ“My Certifications",
    "section": "2. <u>Generative AI Engineering with LLMs Specialization</u>",
    "text": "2. &lt;u&gt;Generative AI Engineering with LLMs Specialization&lt;/u&gt;\n\n1. Architecture and Data\n2. Gen AI Foundational Models for NLP & Language\n3. Generative AI Language Modeling\n4. Generative AI Engineering and Fine-Tuning Transformers\n5. Generative AI Advanced Fine-Tuning for LLMs\n6. Fundamentals of AI Agents Using RAG and LangChain"
  },
  {
    "objectID": "certifications/certificates.html#h21.-uibm-data-science-professional-certificateuh2",
    "href": "certifications/certificates.html#h21.-uibm-data-science-professional-certificateuh2",
    "title": "ðŸŽ“My Certifications",
    "section": "",
    "text": "What is Data Science\nTools for Data Science\nData Science Methodology\nData Analysis with Python\nDatabases and SQL with Python\nMachine Learning with Python\nGenerative AI\n\n\n##\n\n\n1. Architecture and Data\n2. Gen AI Foundational Models for NLP & Language\n3. Generative AI Language Modeling\n4. Generative AI Engineering and Fine-Tuning Transformers\n5. Generative AI Advanced Fine-Tuning for LLMs\n6. Fundamentals of AI Agents Using RAG and LangChain"
  }
]